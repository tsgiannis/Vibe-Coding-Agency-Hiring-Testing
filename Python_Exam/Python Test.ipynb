{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57bda6d",
   "metadata": {},
   "source": [
    "# Python Technical Interview - AI Agent Developer Position\n",
    "\n",
    "## Instructions\n",
    "This notebook contains 10 questions designed to test your Python skills and ability to work with AI-generated code. Each question has:\n",
    "- **Problem Description** - What you need to accomplish\n",
    "- **Code Cell** - Where you write your solution\n",
    "- **Test Cell** - Automated tests to verify your solution\n",
    "\n",
    "**Guidelines:**\n",
    "- Read each question carefully\n",
    "- You can use whatever libraries or packages\n",
    "- Some questions provide starter code, others start from scratch\n",
    "- Focus on writing clean, readable, and robust code\n",
    "- code should be able to run after clearing all outputs\n",
    "- All test cells should pass when you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2500ce",
   "metadata": {},
   "source": [
    "## Question 1: Debug AI-Generated Code (Lists & Logic)\n",
    "\n",
    "**Scenario:** An AI generated this code to filter products by price range, but it has several bugs. Fix the code so it works correctly.\n",
    "\n",
    "**Requirements:**\n",
    "- Filter products where price is between min_price and max_price (inclusive)\n",
    "- Handle edge cases gracefully\n",
    "- Maintain the original function signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fab52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered products: [{'name': 'Mouse', 'price': 25}, {'name': 'Keyboard', 'price': 75}, {'name': 'Monitor', 'price': 300}]\n"
     ]
    }
   ],
   "source": [
    "def filter_products_by_price(products, min_price, max_price):\n",
    "    \"\"\"\n",
    "    Filter products by price range.\n",
    "    \n",
    "    Args:\n",
    "        products: List of dicts with 'name' and 'price' keys\n",
    "        min_price: Minimum price (inclusive)\n",
    "        max_price: Maximum price (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of products within price range\n",
    "    \"\"\"\n",
    "    # AI-generated buggy code below - FIX IT\n",
    "    filtered = []\n",
    "    \n",
    "    # Validate that min_price and max_price are numbers\n",
    "    if not isinstance(min_price, (int, float)) or not isinstance(max_price, (int, float)):\n",
    "        raise ValueError(\"min_price and max_price must be numbers\")\n",
    "    \n",
    "    for product in products:\n",
    "        # Check if product has a price and it's a number\n",
    "        if 'price' not in product or not isinstance(product['price'], (int, float)):\n",
    "            continue  # Skip products with missing or non-numeric prices\n",
    "        \n",
    "        # Check if product price is between min_price and max_price (inclusive)\n",
    "        if product['price'] >= min_price and product['price'] <= max_price:\n",
    "            filtered.append(product)\n",
    "    return filtered\n",
    "\n",
    "# Test your solution here\n",
    "products = [\n",
    "    {'name': 'Laptop', 'price': 1000},\n",
    "    {'name': 'Mouse', 'price': 25},\n",
    "    {'name': 'Keyboard', 'price': 75},\n",
    "    {'name': 'Monitor', 'price': 300},\n",
    "    {'name': 'Adapter', 'price': 'fifteen'},  # Invalid price\n",
    "    {'name': 'Cable'},  # Missing price\n",
    "]\n",
    "\n",
    "result = filter_products_by_price(products, 25, 300)\n",
    "print(\"Filtered products:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8d6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 1 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_1():\n",
    "    products = [\n",
    "        {'name': 'Laptop', 'price': 1000},\n",
    "        {'name': 'Mouse', 'price': 25},\n",
    "        {'name': 'Keyboard', 'price': 75},\n",
    "        {'name': 'Monitor', 'price': 300}\n",
    "    ]\n",
    "    \n",
    "    # Test inclusive bounds\n",
    "    result = filter_products_by_price(products, 25, 300)\n",
    "    expected_names = ['Mouse', 'Keyboard', 'Monitor']\n",
    "    actual_names = [p['name'] for p in result]\n",
    "    assert set(actual_names) == set(expected_names), f\"Expected {expected_names}, got {actual_names}\"\n",
    "    \n",
    "    # Test edge case - empty list\n",
    "    assert filter_products_by_price([], 0, 100) == []\n",
    "    \n",
    "    # Test no matches\n",
    "    assert filter_products_by_price(products, 2000, 3000) == []\n",
    "    \n",
    "    print(\"✓ Question 1 tests passed!\")\n",
    "\n",
    "test_question_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1256e",
   "metadata": {},
   "source": [
    "## Question 2: Fix API Integration (Error Handling)\n",
    "\n",
    "**Scenario:** This AI-generated code fetches user data from an API but lacks proper error handling. Add robust error handling and improve the code.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle network timeouts\n",
    "- Handle HTTP errors (4xx, 5xx)\n",
    "- Handle JSON parsing errors\n",
    "- Return None on any error, don't let exceptions bubble up\n",
    "- Add appropriate logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964dbc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Fetching user data for user_id: 1\n",
      "INFO:__main__:Successfully fetched user data for user_id: 1\n",
      "INFO:__main__:Fetching user data for user_id: 999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User data: {'id': 1, 'name': 'Leanne Graham', 'username': 'Bret', 'email': 'Sincere@april.biz', 'address': {'street': 'Kulas Light', 'suite': 'Apt. 556', 'city': 'Gwenborough', 'zipcode': '92998-3874', 'geo': {'lat': '-37.3159', 'lng': '81.1496'}}, 'phone': '1-770-736-8031 x56442', 'website': 'hildegard.org', 'company': {'name': 'Romaguera-Crona', 'catchPhrase': 'Multi-layered client-server neural-net', 'bs': 'harness real-time e-markets'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:HTTP error Unknown for user_id: 999: 404 Client Error: Not Found for url: https://jsonplaceholder.typicode.com/users/999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing invalid user: None\n",
      "Testing invalid URL scenario with different function...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_user_data(user_id):\n",
    "    \"\"\"\n",
    "    Fetch user data from API with proper error handling.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to fetch\n",
    "        \n",
    "    Returns:\n",
    "        dict: User data if successful, None if any error occurs\n",
    "    \"\"\"\n",
    "    # AI-generated code with poor error handling - IMPROVE IT\n",
    "    url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Fetching user data for user_id: {user_id}\")\n",
    "        \n",
    "        # Make request with timeout\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Check for HTTP errors (4xx, 5xx)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON with explicit error handling\n",
    "        data = response.json()\n",
    "        \n",
    "        logger.info(f\"Successfully fetched user data for user_id: {user_id}\")\n",
    "        return data\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Request timeout for user_id: {user_id}\")\n",
    "        return None\n",
    "        \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Connection error for user_id: {user_id}\")\n",
    "        return None\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        status_code = e.response.status_code if e.response else 'Unknown'\n",
    "        logger.error(f\"HTTP error {status_code} for user_id: {user_id}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request exception for user_id: {user_id}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"JSON parsing error for user_id: {user_id}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error for user_id: {user_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test your solution here\n",
    "user_data = get_user_data(1)\n",
    "print(\"User data:\", user_data)\n",
    "\n",
    "# Test error cases\n",
    "print(\"Testing invalid user:\", get_user_data(999))\n",
    "print(\"Testing invalid URL scenario with different function...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7b4f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Fetching user data for user_id: 1\n",
      "INFO:__main__:Successfully fetched user data for user_id: 1\n",
      "INFO:__main__:Fetching user data for user_id: 999999\n",
      "ERROR:__main__:HTTP error Unknown for user_id: 999999: 404 Client Error: Not Found for url: https://jsonplaceholder.typicode.com/users/999999\n",
      "INFO:__main__:Fetching user data for user_id: 1\n",
      "ERROR:__main__:Request exception for user_id: 1: Network error\n",
      "INFO:__main__:Fetching user data for user_id: 1\n",
      "ERROR:__main__:Request timeout for user_id: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 2 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "import unittest.mock as mock\n",
    "\n",
    "def test_question_2():\n",
    "    # Test successful request\n",
    "    user_data = get_user_data(1)\n",
    "    assert user_data is not None\n",
    "    assert 'name' in user_data\n",
    "    \n",
    "    # Test invalid user ID\n",
    "    user_data = get_user_data(999999)\n",
    "    assert user_data is None\n",
    "    \n",
    "    # Test with mock to simulate network error\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.RequestException(\"Network error\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    # Test with mock to simulate timeout\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.Timeout(\"Timeout\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    print(\"✓ Question 2 tests passed!\")\n",
    "\n",
    "test_question_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a7f14",
   "metadata": {},
   "source": [
    "## Question 3: Code from Scratch (Data Structures)\n",
    "\n",
    "**Scenario:** Create a `TaskManager` class to manage a simple todo list.\n",
    "\n",
    "**Requirements:**\n",
    "- Add tasks with priority (1=high, 2=medium, 3=low)\n",
    "- Mark tasks as complete\n",
    "- Get tasks filtered by completion status and/or priority\n",
    "- Get task count by status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a40e690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks: 3\n",
      "High priority tasks: 1\n",
      "Pending tasks: 3\n"
     ]
    }
   ],
   "source": [
    "class TaskManager:\n",
    "    \"\"\"\n",
    "    A simple task manager for tracking todo items.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty task manager.\"\"\"\n",
    "        self.tasks = []\n",
    "        self.next_id = 1\n",
    "    \n",
    "    def add_task(self, description, priority=2):\n",
    "        \"\"\"\n",
    "        Add a new task.\n",
    "        \n",
    "        Args:\n",
    "            description (str): Task description\n",
    "            priority (int): Priority level (1=high, 2=medium, 3=low)\n",
    "        \"\"\"\n",
    "        # Validate priority\n",
    "        if priority not in (1, 2, 3):\n",
    "            raise ValueError(\"Priority must be 1 (high), 2 (medium), or 3 (low)\")\n",
    "        \n",
    "        task = {\n",
    "            'id': self.next_id,\n",
    "            'description': description,\n",
    "            'priority': priority,\n",
    "            'completed': False\n",
    "        }\n",
    "        self.tasks.append(task)\n",
    "        self.next_id += 1\n",
    "    \n",
    "    def complete_task(self, task_id):\n",
    "        \"\"\"\n",
    "        Mark a task as complete.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Unique identifier for the task\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if task was found and completed, False otherwise\n",
    "        \"\"\"\n",
    "        for task in self.tasks:\n",
    "            if task['id'] == task_id:\n",
    "                task['completed'] = True\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_tasks(self, completed=None, priority=None):\n",
    "        \"\"\"\n",
    "        Get tasks filtered by status and/or priority.\n",
    "        \n",
    "        Args:\n",
    "            completed (bool, optional): Filter by completion status\n",
    "            priority (int, optional): Filter by priority level\n",
    "            \n",
    "        Returns:\n",
    "            list: List of matching tasks\n",
    "        \"\"\"\n",
    "        filtered_tasks = self.tasks\n",
    "        \n",
    "        # Filter by completion status if specified\n",
    "        if completed is not None:\n",
    "            filtered_tasks = [task for task in filtered_tasks if task['completed'] == completed]\n",
    "        \n",
    "        # Filter by priority if specified\n",
    "        if priority is not None:\n",
    "            if priority not in (1, 2, 3):\n",
    "                raise ValueError(\"Priority must be 1 (high), 2 (medium), or 3 (low)\")\n",
    "            filtered_tasks = [task for task in filtered_tasks if task['priority'] == priority]\n",
    "        \n",
    "        return filtered_tasks\n",
    "    \n",
    "    def get_task_count(self, completed=None):\n",
    "        \"\"\"\n",
    "        Get count of tasks by completion status.\n",
    "        \n",
    "        Args:\n",
    "            completed (bool, optional): Count completed (True) or pending (False) tasks\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of matching tasks\n",
    "        \"\"\"\n",
    "        if completed is None:\n",
    "            return len(self.tasks)\n",
    "        else:\n",
    "            return len([task for task in self.tasks if task['completed'] == completed])\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "tm = TaskManager()\n",
    "tm.add_task(\"Fix bug in login\", 1)  # High priority\n",
    "tm.add_task(\"Update documentation\", 3)  # Low priority\n",
    "tm.add_task(\"Code review\", 2)  # Medium priority\n",
    "\n",
    "print(\"All tasks:\", len(tm.get_tasks()))\n",
    "print(\"High priority tasks:\", len(tm.get_tasks(priority=1)))\n",
    "print(\"Pending tasks:\", len(tm.get_tasks(completed=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d50272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 3 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_3():\n",
    "    tm = TaskManager()\n",
    "    \n",
    "    # Test adding tasks\n",
    "    tm.add_task(\"Task 1\", 1)\n",
    "    tm.add_task(\"Task 2\", 2)\n",
    "    tm.add_task(\"Task 3\", 3)\n",
    "    \n",
    "    # Test get all tasks\n",
    "    all_tasks = tm.get_tasks()\n",
    "    assert len(all_tasks) == 3\n",
    "    \n",
    "    # Test priority filtering\n",
    "    high_priority = tm.get_tasks(priority=1)\n",
    "    assert len(high_priority) == 1\n",
    "    \n",
    "    # Test task completion\n",
    "    task_id = all_tasks[0]['id']  # Assuming tasks have 'id' field\n",
    "    success = tm.complete_task(task_id)\n",
    "    assert success == True\n",
    "    \n",
    "    # Test completion filtering\n",
    "    completed_tasks = tm.get_tasks(completed=True)\n",
    "    assert len(completed_tasks) == 1\n",
    "    \n",
    "    pending_tasks = tm.get_tasks(completed=False)\n",
    "    assert len(pending_tasks) == 2\n",
    "    \n",
    "    # Test task counts\n",
    "    assert tm.get_task_count() == 3\n",
    "    assert tm.get_task_count(completed=True) == 1\n",
    "    assert tm.get_task_count(completed=False) == 2\n",
    "    \n",
    "    print(\"✓ Question 3 tests passed!\")\n",
    "\n",
    "test_question_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243a1dc",
   "metadata": {},
   "source": [
    "## Question 4: Optimize AI Code (Performance)\n",
    "\n",
    "**Scenario:** This AI code finds common elements between multiple lists, but it's very inefficient. Optimize it for better performance.\n",
    "\n",
    "**Requirements:**\n",
    "- Same functionality as original\n",
    "- Significantly better time complexity\n",
    "- Handle edge cases (empty lists, no common elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d39f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow version: [4, 5]\n",
      "Fast version: [4, 5]\n"
     ]
    }
   ],
   "source": [
    "def find_common_elements_slow(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    AI-generated inefficient version - OPTIMIZE THIS!\n",
    "    \n",
    "    Args:\n",
    "        lists: List of lists to find common elements in\n",
    "        \n",
    "    Returns:\n",
    "        list: Elements that appear in all lists\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    common = []\n",
    "    for item in lists[0]:\n",
    "        is_common = True\n",
    "        for other_list in lists[1:]:\n",
    "            found = False\n",
    "            for other_item in other_list:\n",
    "                if item == other_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                is_common = False\n",
    "                break\n",
    "        if is_common and item not in common:\n",
    "            common.append(item)\n",
    "    \n",
    "    return common\n",
    "\n",
    "# Optimized version - implement this\n",
    "def find_common_elements_fast(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    Optimized version with better time complexity.\n",
    "    \n",
    "    Args:\n",
    "        lists: List of lists to find common elements in\n",
    "        \n",
    "    Returns:\n",
    "        list: Elements that appear in all lists\n",
    "    \"\"\"\n",
    "    # Handle edge cases\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    # Convert all lists to sets for O(1) lookups\n",
    "    sets = [set(lst) for lst in lists]\n",
    "    \n",
    "    # Start with the smallest set to minimize intersection operations\n",
    "    smallest_set = min(sets, key=len)\n",
    "    \n",
    "    # Find intersection of all sets\n",
    "    common_elements = smallest_set.copy()\n",
    "    for s in sets:\n",
    "        common_elements &= s  # Set intersection operation\n",
    "    \n",
    "    # Convert back to list and return\n",
    "    return list(common_elements)\n",
    "# Test both versions\n",
    "test_lists = [\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [3, 4, 5, 6, 7],\n",
    "    [4, 5, 7, 8, 9]\n",
    "]\n",
    "\n",
    "print(\"Slow version:\", find_common_elements_slow(test_lists))\n",
    "print(\"Fast version:\", find_common_elements_fast(test_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de081d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 4 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "\n",
    "import time\n",
    "\n",
    "def test_question_4():\n",
    "    # Basic functionality test\n",
    "    test_lists = [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [3, 4, 5, 6, 7],\n",
    "        [4, 5, 7, 8, 9]\n",
    "    ]\n",
    "    \n",
    "    slow_result = find_common_elements_slow(test_lists)\n",
    "    fast_result = find_common_elements_fast(test_lists)\n",
    "    \n",
    "    assert set(slow_result) == set(fast_result), \"Results don't match\"\n",
    "    assert set(fast_result) == {4, 5}, f\"Expected {{4, 5}}, got {set(fast_result)}\"\n",
    "    \n",
    "    # Edge cases\n",
    "    assert find_common_elements_fast([]) == []\n",
    "    assert find_common_elements_fast([[1, 2], []]) == []\n",
    "    assert find_common_elements_fast([[1, 2, 3]]) == [1, 2, 3]\n",
    "    \n",
    "    # Performance test (rough)\n",
    "    large_lists = [[i for i in range(1000)] for _ in range(10)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    find_common_elements_fast(large_lists)\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    # Fast version should complete in reasonable time\n",
    "    assert fast_time < 1.0, \"Optimized version is still too slow\"\n",
    "    \n",
    "    print(\"✓ Question 4 tests passed!\")\n",
    "\n",
    "test_question_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7131a24",
   "metadata": {},
   "source": [
    "## Question 5: Fix Function with Edge Cases\n",
    "\n",
    "**Scenario:** This AI function calculates statistics for a list of numbers, but fails on various edge cases. Make it robust.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle empty lists\n",
    "- Handle non-numeric values gracefully\n",
    "- Handle division by zero\n",
    "- Return meaningful error messages or default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ffe4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(numbers):\n",
    "    \"\"\"\n",
    "    Calculate basic statistics for a list of numbers.\n",
    "    Fixed version with proper mode calculation.\n",
    "    \"\"\"\n",
    "    # Handle empty list\n",
    "    if not numbers:\n",
    "        return {\n",
    "            'mean': None,\n",
    "            'median': None,\n",
    "            'mode': None,\n",
    "            'std_dev': None,\n",
    "            'count': 0,\n",
    "            'error': 'Empty list provided'\n",
    "        }\n",
    "    \n",
    "    # Filter out non-numeric values\n",
    "    numeric_numbers = []\n",
    "    non_numeric_count = 0\n",
    "    \n",
    "    for num in numbers:\n",
    "        if isinstance(num, (int, float)) and num is not None:\n",
    "            numeric_numbers.append(num)\n",
    "        else:\n",
    "            non_numeric_count += 1\n",
    "    \n",
    "    # Handle case where no valid numbers remain\n",
    "    if not numeric_numbers:\n",
    "        return {\n",
    "            'mean': None,\n",
    "            'median': None,\n",
    "            'mode': None,\n",
    "            'std_dev': None,\n",
    "            'count': 0,\n",
    "            'error': 'No valid numeric values found'\n",
    "        }\n",
    "    \n",
    "    # Handle case with only one valid number\n",
    "    if len(numeric_numbers) == 1:\n",
    "        single_value = numeric_numbers[0]\n",
    "        return {\n",
    "            'mean': single_value,\n",
    "            'median': single_value,\n",
    "            'mode': single_value,\n",
    "            'std_dev': 0.0,\n",
    "            'count': 1,\n",
    "            'non_numeric_count': non_numeric_count\n",
    "        }\n",
    "    \n",
    "    # Sort for median calculation\n",
    "    sorted_nums = sorted(numeric_numbers)\n",
    "    n = len(numeric_numbers)\n",
    "    \n",
    "    # Mean\n",
    "    mean = sum(numeric_numbers) / n\n",
    "    \n",
    "    # Median\n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_nums[n//2]\n",
    "    \n",
    "    # Mode - FIXED VERSION\n",
    "    from collections import Counter\n",
    "    counts = Counter(numeric_numbers)\n",
    "    \n",
    "    # Get the most common element (this returns a list of (element, count) tuples)\n",
    "    most_common = counts.most_common(1)\n",
    "    \n",
    "    # The mode is the element with the highest frequency\n",
    "    mode = most_common[0][0]  # First element of first tuple\n",
    "    \n",
    "    # Standard deviation\n",
    "    variance = sum((x - mean) ** 2 for x in numeric_numbers) / n\n",
    "    std_dev = variance ** 0.5\n",
    "    \n",
    "    # Build result\n",
    "    result = {\n",
    "        'mean': round(mean, 4),\n",
    "        'median': round(median, 4),\n",
    "        'mode': mode,  # Now correctly shows most frequent value\n",
    "        'std_dev': round(std_dev, 4),\n",
    "        'count': n\n",
    "    }\n",
    "    \n",
    "    # Add non-numeric count if any were filtered out\n",
    "    if non_numeric_count > 0:\n",
    "        result['non_numeric_count'] = non_numeric_count\n",
    "        result['valid_count'] = n\n",
    "        result['total_count'] = len(numbers)\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b590186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 5 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_5():\n",
    "    # Normal case\n",
    "    result = calculate_stats([1, 2, 3, 4, 5])\n",
    "    assert result['mean'] == 3.0\n",
    "    assert result['median'] == 3.0\n",
    "    assert result['count'] == 5\n",
    "    \n",
    "    # Single item\n",
    "    result = calculate_stats([42])\n",
    "    assert result['mean'] == 42\n",
    "    assert result['median'] == 42\n",
    "    assert result['mode'] == 42\n",
    "    assert result['std_dev'] == 0\n",
    "    \n",
    "    # Empty list - should handle gracefully\n",
    "    result = calculate_stats([])\n",
    "    assert 'error' in result or all(v is None or v == 0 for v in result.values())\n",
    "    \n",
    "    # Mixed types - should handle gracefully\n",
    "    result = calculate_stats([1, 'invalid', 3])\n",
    "    assert 'error' in result or result['count'] == 2  # Only valid numbers counted\n",
    "    \n",
    "    # All same values\n",
    "    result = calculate_stats([5, 5, 5, 5])\n",
    "    assert result['mean'] == 5\n",
    "    assert result['std_dev'] == 0\n",
    "    \n",
    "    print(\"✓ Question 5 tests passed!\")\n",
    "\n",
    "test_question_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c594b33",
   "metadata": {},
   "source": [
    "## Question 6: Complete Partial Implementation (Pandas/Data)\n",
    "\n",
    "### Goal\n",
    "Implement `analyze_sales_data(df, group_by_column)`.\n",
    "\n",
    "### Input\n",
    "A pandas DataFrame `df` with columns:\n",
    "- `product`\n",
    "- `category`\n",
    "- `sales`\n",
    "- `profit`\n",
    "\n",
    "### Output (must match exactly)\n",
    "- Return a DataFrame **indexed by `group_by_column`** (do not reset the index).\n",
    "- Include exactly these columns (names must match):\n",
    "  - `sales_sum` — sum of `sales`\n",
    "  - `sales_mean` — mean of `sales`\n",
    "  - `profit_sum` — sum of `profit`\n",
    "  - `profit_mean` — mean of `profit`\n",
    "  - `profit_margin` — `profit_sum / sales_sum` (use `NaN` if `sales_sum == 0`)\n",
    "- Handle missing values: treat missing `sales` or `profit` as `0` before aggregation.\n",
    "- Sorting is **not required**.\n",
    "\n",
    "### Edge Behavior\n",
    "- If `df` is empty or `group_by_column` is missing, return an empty DataFrame with the required column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e96db00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "  product     category  sales  profit\n",
      "0       A  Electronics  100.0      20\n",
      "1       B  Electronics  200.0      50\n",
      "2       C     Clothing  150.0      30\n",
      "3       A  Electronics  120.0      25\n",
      "4       B  Electronics    NaN      40\n",
      "5       C     Clothing  180.0      35\n",
      "6       A  Electronics  110.0      22\n",
      "\n",
      "Analysis by product:\n",
      "         sales_sum  sales_mean  profit_sum  profit_mean  profit_margin\n",
      "product                                                               \n",
      "A            330.0       110.0          67    22.333333        0.20303\n",
      "B            200.0       100.0          90    45.000000        0.45000\n",
      "C            330.0       165.0          65    32.500000        0.19697\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_sales_data(df, group_by_column):\n",
    "    \"\"\"\n",
    "    Analyze sales data by grouping and calculating statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['product', 'category', 'sales', 'profit']\n",
    "        group_by_column: Column name to group by\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with aggregated statistics\n",
    "    \"\"\"\n",
    "    # Handle edge cases: empty DataFrame or missing group_by_column\n",
    "    if df.empty or group_by_column not in df.columns:\n",
    "        # Return empty DataFrame with required columns\n",
    "        return pd.DataFrame(columns=['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin'])\n",
    "    \n",
    "    # Create a copy to avoid modifying original DataFrame\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handle missing values: treat missing sales or profit as 0\n",
    "    df_clean['sales'] = df_clean['sales'].fillna(0)\n",
    "    df_clean['profit'] = df_clean['profit'].fillna(0)\n",
    "    \n",
    "    # Group by the specified column\n",
    "    grouped = df_clean.groupby(group_by_column)\n",
    "    \n",
    "    # Calculate required aggregations\n",
    "    result = grouped.agg({\n",
    "        'sales': ['sum', 'mean'],\n",
    "        'profit': ['sum', 'mean']\n",
    "    })\n",
    "    \n",
    "    # Flatten the multi-level column names\n",
    "    result.columns = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean']\n",
    "    \n",
    "    # Calculate profit margin (profit_sum / sales_sum), handle division by zero\n",
    "    result['profit_margin'] = np.where(\n",
    "        result['sales_sum'] == 0, \n",
    "        np.nan, \n",
    "        result['profit_sum'] / result['sales_sum']\n",
    "    )\n",
    "    \n",
    "    # Select and return only the required columns in the specified order\n",
    "    return result[['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']]\n",
    "\n",
    "# Create sample data for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],\n",
    "    'category': ['Electronics', 'Electronics', 'Clothing', 'Electronics', 'Electronics', 'Clothing', 'Electronics'],\n",
    "    'sales': [100, 200, 150, 120, np.nan, 180, 110],\n",
    "    'profit': [20, 50, 30, 25, 40, 35, 22]\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(sample_data)\n",
    "print(\"\\nAnalysis by product:\")\n",
    "result = analyze_sales_data(sample_data, 'product')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a0c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 6 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_6():\n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'product': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'category': ['Cat1', 'Cat2', 'Cat1', 'Cat2', 'Cat1'],\n",
    "        'sales': [100, 200, 150, 300, 50],\n",
    "        'profit': [20, 40, 30, 60, 10]\n",
    "    })\n",
    "    \n",
    "    # Test grouping by product\n",
    "    result = analyze_sales_data(test_data, 'product')\n",
    "    \n",
    "    # Check structure\n",
    "    assert isinstance(result, pd.DataFrame), \"Should return DataFrame\"\n",
    "    assert len(result) == 2, \"Should have 2 groups (A and B)\"\n",
    "    \n",
    "    # Check required columns exist\n",
    "    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    for col in required_cols:\n",
    "        assert col in result.columns, f\"Missing column: {col}\"\n",
    "    \n",
    "    # Check calculations for product A\n",
    "    product_a = result.loc['A'] if 'A' in result.index else result[result.index == 'A'].iloc[0]\n",
    "    assert product_a['sales_sum'] == 300, \"Product A sales sum should be 300\"\n",
    "    assert product_a['profit_sum'] == 60, \"Product A profit sum should be 60\"\n",
    "    \n",
    "    print(\"✓ Question 6 tests passed!\")\n",
    "\n",
    "test_question_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b18d21",
   "metadata": {},
   "source": [
    "## Question 7: Refactor Messy AI Code (Clean Code)\n",
    "\n",
    "**Scenario:** This AI code works but is poorly structured and hard to maintain. Refactor it following clean code principles.\n",
    "\n",
    "**Requirements:**\n",
    "- Improve readability and maintainability\n",
    "- Add proper documentation\n",
    "- Follow naming conventions\n",
    "- Break down large functions\n",
    "- Add type hints if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db7e3a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original result: {'adult': {'count': 2, 'emails': ['user1@test.com', 'user4@test.com'], 'avg_age': 32.5}, 'senior': {'count': 1, 'emails': ['user2@test.com'], 'avg_age': 70.0}}\n",
      "Clean result: {'adult': {'count': 2, 'emails': ['user1@test.com', 'user4@test.com'], 'avg_age': 32.5}, 'senior': {'count': 1, 'emails': ['user2@test.com'], 'avg_age': 70.0}}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "def process_data(data):\n",
    "    \"\"\"Messy AI-generated code that works but needs refactoring - CLEAN IT UP!\"\"\"\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        if 'type' in item and item['type'] == 'user':\n",
    "            if 'active' in item and item['active']:\n",
    "                if 'age' in item:\n",
    "                    if item['age'] >= 18:\n",
    "                        if 'email' in item and '@' in item['email']:\n",
    "                            category = 'adult'\n",
    "                            if item['age'] >= 65:\n",
    "                                category = 'senior'\n",
    "                            elif item['age'] >= 25:\n",
    "                                category = 'adult'\n",
    "                            else:\n",
    "                                category = 'young_adult'\n",
    "                            \n",
    "                            if category not in result:\n",
    "                                result[category] = {'count': 0, 'emails': [], 'total_age': 0}\n",
    "                            \n",
    "                            result[category]['count'] += 1\n",
    "                            result[category]['emails'].append(item['email'])\n",
    "                            result[category]['total_age'] += item['age']\n",
    "    \n",
    "    # Calculate averages\n",
    "    for cat in result:\n",
    "        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']\n",
    "        del result[cat]['total_age']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test data\n",
    "test_data = [\n",
    "    {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "    {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "    {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "    {'type': 'admin', 'active': True, 'age': 35, 'email': 'admin@test.com'},\n",
    "    {'type': 'user', 'active': True, 'age': 20, 'email': 'invalid-email'},\n",
    "    {'type': 'user', 'active': True, 'age': 40, 'email': 'user4@test.com'},\n",
    "]\n",
    "\n",
    "# Your refactored version should produce the same results\n",
    "original_result = process_data(test_data)\n",
    "print(\"Original result:\", original_result)\n",
    "\n",
    "# TODO: Create your clean, refactored version here\n",
    "def process_user_data_clean(data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process user data to categorize active adult users by age groups.\n",
    "    \n",
    "    Refactored with clean code principles and constant age thresholds.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing user data with keys:\n",
    "              'type', 'active', 'age', 'email'\n",
    "              \n",
    "    Returns:\n",
    "        Dictionary with age categories as keys, each containing:\n",
    "        - count: Number of users in category\n",
    "        - emails: List of user emails\n",
    "        - avg_age: Average age in category\n",
    "    \"\"\"\n",
    "    # Age threshold constants - easy to modify in one place\n",
    "    YOUNG_ADULT_MIN_AGE = 18\n",
    "    ADULT_MIN_AGE = 25\n",
    "    SENIOR_MIN_AGE = 65\n",
    "    \n",
    "    # Calculate upper bounds (one less than next threshold)\n",
    "    YOUNG_ADULT_MAX_AGE = ADULT_MIN_AGE - 1      # 24\n",
    "    ADULT_MAX_AGE = SENIOR_MIN_AGE - 1           # 64\n",
    "    # Senior has no upper limit\n",
    "    \n",
    "    # Configuration using constants\n",
    "    AGE_CATEGORIES = {\n",
    "        'young_adult': {'min_age': YOUNG_ADULT_MIN_AGE, 'max_age': YOUNG_ADULT_MAX_AGE},\n",
    "        'adult': {'min_age': ADULT_MIN_AGE, 'max_age': ADULT_MAX_AGE},\n",
    "        'senior': {'min_age': SENIOR_MIN_AGE, 'max_age': None}  # No upper limit\n",
    "    }\n",
    "    \n",
    "    def is_valid_user(user: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if user meets all validation criteria.\n",
    "        \n",
    "        Args:\n",
    "            user: User data dictionary\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if user is valid for processing\n",
    "        \"\"\"\n",
    "        # Check required fields exist\n",
    "        if not all(field in user for field in ['type', 'active', 'age', 'email']):\n",
    "            return False\n",
    "        \n",
    "        # Check user type and active status\n",
    "        if user['type'] != 'user' or not user['active']:\n",
    "            return False\n",
    "        \n",
    "        # Check minimum age using constant\n",
    "        if user['age'] < YOUNG_ADULT_MIN_AGE:\n",
    "            return False\n",
    "        \n",
    "        # Check email format\n",
    "        return '@' in user['email']\n",
    "    \n",
    "    def get_age_category(age: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Determine which age category the user belongs to using constants.\n",
    "        \n",
    "        Args:\n",
    "            age: User's age\n",
    "            \n",
    "        Returns:\n",
    "            str: Age category name, or None if no category matches\n",
    "        \"\"\"\n",
    "        # Use constant-based logic for clarity\n",
    "        if age >= SENIOR_MIN_AGE:\n",
    "            return 'senior'\n",
    "        elif age >= ADULT_MIN_AGE:\n",
    "            return 'adult'\n",
    "        elif age >= YOUNG_ADULT_MIN_AGE:\n",
    "            return 'young_adult'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def initialize_category(category: str, results: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Ensure a category exists in results with proper structure.\n",
    "        \n",
    "        Args:\n",
    "            category: Category name to initialize\n",
    "            results: Results dictionary to modify\n",
    "        \"\"\"\n",
    "        if category not in results:\n",
    "            results[category] = {\n",
    "                'count': 0,\n",
    "                'emails': [],\n",
    "                'total_age': 0\n",
    "            }\n",
    "    \n",
    "    def process_valid_user(user: Dict[str, Any], results: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Process a single valid user and update results.\n",
    "        \n",
    "        Args:\n",
    "            user: Valid user data\n",
    "            results: Results dictionary to update\n",
    "        \"\"\"\n",
    "        age = user['age']\n",
    "        email = user['email']\n",
    "        category = get_age_category(age)\n",
    "        \n",
    "        if category:\n",
    "            initialize_category(category, results)\n",
    "            \n",
    "            results[category]['count'] += 1\n",
    "            results[category]['emails'].append(email)\n",
    "            results[category]['total_age'] += age\n",
    "    \n",
    "    def calculate_final_statistics(results: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate averages and clean up temporary fields.\n",
    "        \n",
    "        Args:\n",
    "            results: Results dictionary with raw data\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Results with averages calculated\n",
    "        \"\"\"\n",
    "        final_results = {}\n",
    "        \n",
    "        for category, stats in results.items():\n",
    "            final_results[category] = {\n",
    "                'count': stats['count'],\n",
    "                'emails': stats['emails'],\n",
    "                'avg_age': round(stats['total_age'] / stats['count'], 2) if stats['count'] > 0 else 0\n",
    "            }\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    # Main processing logic\n",
    "    raw_results = {}\n",
    "    \n",
    "    for user in data:\n",
    "        if is_valid_user(user):\n",
    "            process_valid_user(user, raw_results)\n",
    "    \n",
    "    return calculate_final_statistics(raw_results)\n",
    "\n",
    "# Test both versions\n",
    "clean_result = process_user_data_clean(test_data)\n",
    "print(\"Clean result:\", clean_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93e1ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 7 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_7():\n",
    "    test_data = [\n",
    "        {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'},\n",
    "        {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'},\n",
    "        {'type': 'user', 'active': True, 'age': 20, 'email': 'user4@test.com'},\n",
    "    ]\n",
    "    \n",
    "    original_result = process_data(test_data)\n",
    "    clean_result = process_user_data_clean(test_data)\n",
    "    \n",
    "    # Results should be functionally equivalent\n",
    "    assert set(original_result.keys()) == set(clean_result.keys()), \"Categories don't match\"\n",
    "    \n",
    "    for category in original_result:\n",
    "        assert original_result[category]['count'] == clean_result[category]['count'], f\"Count mismatch for {category}\"\n",
    "        assert abs(original_result[category]['avg_age'] - clean_result[category]['avg_age']) < 0.01, f\"Average age mismatch for {category}\"\n",
    "    \n",
    "    print(\"✓ Question 7 tests passed!\")\n",
    "\n",
    "test_question_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf968be",
   "metadata": {},
   "source": [
    "## Question 8: Debug Complex Logic (Algorithms)\n",
    "\n",
    "**Scenario:** This AI implementation of binary search has subtle bugs. Find and fix all the issues.\n",
    "\n",
    "**Requirements:**\n",
    "- Fix the binary search algorithm\n",
    "- Handle edge cases properly\n",
    "- Maintain O(log n) time complexity\n",
    "- Return correct index or -1 if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3b759b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 7 in [1, 3, 5, 7, 9, 11]: 3\n",
      "Searching for 1 in [1, 3, 5, 7, 9, 11]: 0\n",
      "Searching for 11 in [1, 3, 5, 7, 9, 11]: 5\n",
      "Searching for 6 in [1, 3, 5, 7, 9, 11]: -1\n",
      "Searching for 5 in [5]: 0\n",
      "Searching for 3 in [5]: -1\n",
      "Searching for 5 in []: -1\n"
     ]
    }
   ],
   "source": [
    "def binary_search_buggy(arr, target):\n",
    "    \"\"\"\n",
    "    Fixed binary search implementation.\n",
    "    Kept the naming/structure of the original buggy code but fixed the issues.\n",
    "    Args:\n",
    "        arr: Sorted list of integers\n",
    "        target: Value to search for\n",
    "        \n",
    "    Returns:\n",
    "        int: Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    # Handle empty array edge case\n",
    "    if not arr:\n",
    "        return -1\n",
    "    \n",
    "    left = 0\n",
    "    right = len(arr) - 1  # FIX 1: right should be last index, not length\n",
    "    \n",
    "    while left <= right:  # FIX 2: Use <= to handle single element case\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1  # FIX 3: Exclude mid from next search\n",
    "        else:\n",
    "            right = mid - 1  # FIX 4: Exclude mid from next search\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# Test cases\n",
    "test_arrays = [\n",
    "    ([1, 3, 5, 7, 9, 11], 7),    # Should find at index 3\n",
    "    ([1, 3, 5, 7, 9, 11], 1),    # Should find at index 0\n",
    "    ([1, 3, 5, 7, 9, 11], 11),   # Should find at index 5\n",
    "    ([1, 3, 5, 7, 9, 11], 6),    # Should return -1\n",
    "    ([5], 5),                     # Single element found\n",
    "    ([5], 3),                     # Single element not found\n",
    "    ([], 5),                      # Empty array\n",
    "]\n",
    "\n",
    "for arr, target in test_arrays:\n",
    "    result = binary_search_buggy(arr, target)\n",
    "    print(f\"Searching for {target} in {arr}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e288e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Question 8 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_8():\n",
    "    # Test cases with expected results\n",
    "    test_cases = [\n",
    "        ([1, 3, 5, 7, 9, 11], 7, 3),      # Found at index 3\n",
    "        ([1, 3, 5, 7, 9, 11], 1, 0),      # Found at index 0\n",
    "        ([1, 3, 5, 7, 9, 11], 11, 5),     # Found at index 5\n",
    "        ([1, 3, 5, 7, 9, 11], 6, -1),     # Not found\n",
    "        ([1, 3, 5, 7, 9, 11], 0, -1),     # Less than min\n",
    "        ([1, 3, 5, 7, 9, 11], 12, -1),    # Greater than max\n",
    "        ([5], 5, 0),                       # Single element found\n",
    "        ([5], 3, -1),                      # Single element not found\n",
    "        ([], 5, -1),                       # Empty array\n",
    "    ]\n",
    "    \n",
    "    for arr, target, expected in test_cases:\n",
    "        result = binary_search_buggy(arr, target)\n",
    "        assert result == expected, f\"Failed for {target} in {arr}: expected {expected}, got {result}\"\n",
    "    \n",
    "    # Test that it actually uses binary search (check performance)\n",
    "    large_array = list(range(0, 10000, 2))  # [0, 2, 4, 6, ..., 9998]\n",
    "    result = binary_search_buggy(large_array, 5000)\n",
    "    assert result == 2500, \"Should find 5000 at index 2500\"\n",
    "    \n",
    "    print(\"✓ Question 8 tests passed!\")\n",
    "\n",
    "test_question_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ecefa",
   "metadata": {},
   "source": [
    "## Question 9: Add Missing Functionality\n",
    "\n",
    "**Scenario:** This AI code provides a basic cache implementation but is missing several key features. Add the missing functionality to make it production-ready.\n",
    "\n",
    "**Requirements:**\n",
    "- Add TTL (time-to-live) support for automatic expiration\n",
    "- Add size limit with LRU (Least Recently Used) eviction\n",
    "- Add cache statistics tracking (hits, misses, evictions)\n",
    "- Add methods for cache management (clear, size, cleanup)\n",
    "- Handle thread safety considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cafdbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing TTL ===\n",
      "Immediately after set: temp_value\n",
      "After TTL expired: None\n",
      "\n",
      "=== Testing Size Limits & LRU ===\n",
      "Cache size after adding 3 items: 3\n",
      "After adding 'd': a=1, b=None, c=3, d=4\n",
      "\n",
      "=== Testing Statistics ===\n",
      "Cache statistics: {'hits': 5, 'misses': 2, 'evictions': 1, 'expired_removals': 0, 'current_size': 3}\n",
      "\n",
      "=== Testing Cleanup ===\n",
      "Expired items removed: 3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "from typing import Any, Optional, Dict\n",
    "from collections import OrderedDict\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"\n",
    "    Enhanced cache with TTL, LRU eviction, statistics, and thread safety.\n",
    "    \n",
    "    Features:\n",
    "    - TTL (time-to-live) support with automatic expiration\n",
    "    - Size limit with LRU (Least Recently Used) eviction\n",
    "    - Cache statistics tracking (hits, misses, evictions)\n",
    "    - Cache management methods (clear, size, cleanup)\n",
    "    - Thread-safe operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize cache with size limit and default TTL.\n",
    "        \n",
    "        Args:\n",
    "            max_size: Maximum number of items to store\n",
    "            default_ttl: Default time-to-live in seconds (None = no expiration)\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.default_ttl = default_ttl\n",
    "        \n",
    "        # Main storage: {key: {'value': value, 'expires_at': timestamp}}\n",
    "        self._data = {}\n",
    "        \n",
    "        # LRU tracking using OrderedDict (automatically maintains access order)\n",
    "        self._lru = OrderedDict()\n",
    "        \n",
    "        # Statistics\n",
    "        self._stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'expired_removals': 0\n",
    "        }\n",
    "        \n",
    "        # Thread safety\n",
    "        self._lock = threading.RLock()\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get value from cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "            \n",
    "        Returns:\n",
    "            Cached value or None if not found/expired\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            # Check if key exists and not expired\n",
    "            if self._is_expired(key):\n",
    "                self._remove_key(key)\n",
    "                self._stats['misses'] += 1\n",
    "                return None\n",
    "            \n",
    "            if key in self._data:\n",
    "                # Update LRU order (move to end)\n",
    "                self._lru.move_to_end(key)\n",
    "                self._stats['hits'] += 1\n",
    "                return self._data[key]['value']\n",
    "            else:\n",
    "                self._stats['misses'] += 1\n",
    "                return None\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set value in cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key\n",
    "            value: Value to cache\n",
    "            ttl: Time-to-live in seconds (overrides default)\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            # Check if cache is full and evict if needed\n",
    "            if len(self._data) >= self.max_size and key not in self._data:\n",
    "                self._evict_lru(1)\n",
    "            \n",
    "            # Calculate expiration time\n",
    "            expires_at = None\n",
    "            if ttl is not None:\n",
    "                expires_at = time.time() + ttl\n",
    "            elif self.default_ttl is not None:\n",
    "                expires_at = time.time() + self.default_ttl\n",
    "            \n",
    "            # Store value with metadata\n",
    "            self._data[key] = {\n",
    "                'value': value,\n",
    "                'expires_at': expires_at\n",
    "            }\n",
    "            \n",
    "            # Update LRU order (add or move to end)\n",
    "            self._lru[key] = True\n",
    "            self._lru.move_to_end(key)\n",
    "    \n",
    "    def delete(self, key: str) -> bool:\n",
    "        \"\"\"Delete key from cache.\"\"\"\n",
    "        with self._lock:\n",
    "            return self._remove_key(key)\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all items from cache.\"\"\"\n",
    "        with self._lock:\n",
    "            self._data.clear()\n",
    "            self._lru.clear()\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return current number of items in cache.\"\"\"\n",
    "        with self._lock:\n",
    "            return len(self._data)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get cache statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with keys: hits, misses, evictions, expired_removals, current_size\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            stats = self._stats.copy()\n",
    "            stats['current_size'] = len(self._data)\n",
    "            return stats\n",
    "    \n",
    "    def cleanup_expired(self) -> int:\n",
    "        \"\"\"\n",
    "        Remove expired items from cache.\n",
    "        \n",
    "        Returns:\n",
    "            Number of items removed\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            expired_keys = []\n",
    "            for key in list(self._data.keys()):\n",
    "                if self._is_expired(key):\n",
    "                    expired_keys.append(key)\n",
    "            \n",
    "            for key in expired_keys:\n",
    "                self._remove_key(key)\n",
    "                self._stats['expired_removals'] += 1\n",
    "            \n",
    "            return len(expired_keys)\n",
    "    \n",
    "    def _evict_lru(self, count: int = 1) -> int:\n",
    "        \"\"\"\n",
    "        Evict least recently used items.\n",
    "        \n",
    "        Args:\n",
    "            count: Number of items to evict\n",
    "            \n",
    "        Returns:\n",
    "            Number of items actually evicted\n",
    "        \"\"\"\n",
    "        evicted_count = 0\n",
    "        while self._lru and evicted_count < count:\n",
    "            # Get the least recently used key (first in OrderedDict)\n",
    "            lru_key, _ = self._lru.popitem(last=False)\n",
    "            if lru_key in self._data:\n",
    "                del self._data[lru_key]\n",
    "                evicted_count += 1\n",
    "                self._stats['evictions'] += 1\n",
    "        \n",
    "        return evicted_count\n",
    "    \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if a cache entry has expired.\"\"\"\n",
    "        if key not in self._data:\n",
    "            return False\n",
    "        \n",
    "        entry = self._data[key]\n",
    "        if entry['expires_at'] is None:\n",
    "            return False  # No expiration\n",
    "        \n",
    "        return time.time() > entry['expires_at']\n",
    "    \n",
    "    def _remove_key(self, key: str) -> bool:\n",
    "        \"\"\"Remove key from all data structures.\"\"\"\n",
    "        if key in self._data:\n",
    "            del self._data[key]\n",
    "            if key in self._lru:\n",
    "                del self._lru[key]\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        \"\"\"Check if key exists and is not expired.\"\"\"\n",
    "        with self._lock:\n",
    "            if self._is_expired(key):\n",
    "                self._remove_key(key)\n",
    "                return False\n",
    "            return key in self._data\n",
    "    \n",
    "    def keys(self):\n",
    "        \"\"\"Get all non-expired keys.\"\"\"\n",
    "        with self._lock:\n",
    "            self.cleanup_expired()  # Clean up before returning keys\n",
    "            return list(self._data.keys())\n",
    "\n",
    "# Test your enhanced implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test TTL functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=1)  # 1 second TTL\n",
    "    \n",
    "    print(\"=== Testing TTL ===\")\n",
    "    cache.set(\"temp_key\", \"temp_value\")\n",
    "    print(f\"Immediately after set: {cache.get('temp_key')}\")\n",
    "    time.sleep(1.1)\n",
    "    print(f\"After TTL expired: {cache.get('temp_key')}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Size Limits & LRU ===\")\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1, ttl=None)  # No expiration\n",
    "    cache.set(\"b\", 2, ttl=None)\n",
    "    cache.set(\"c\", 3, ttl=None)\n",
    "    print(f\"Cache size after adding 3 items: {cache.size()}\")\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4, ttl=None)\n",
    "    print(f\"After adding 'd': a={cache.get('a')}, b={cache.get('b')}, c={cache.get('c')}, d={cache.get('d')}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Statistics ===\")\n",
    "    stats = cache.get_stats()\n",
    "    print(f\"Cache statistics: {stats}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Cleanup ===\")\n",
    "    cache.set(\"expire_me\", \"value\", ttl=1)\n",
    "    time.sleep(1.1)\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    print(f\"Expired items removed: {removed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bf0f585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing enhanced cache implementation...\n",
      "✓ All Question 9 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell \n",
    "import time\n",
    "\n",
    "def test_question_9():\n",
    "    print(\"Testing enhanced cache implementation...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=60)\n",
    "    \n",
    "    cache.set(\"key1\", \"value1\")\n",
    "    cache.set(\"key2\", \"value2\")\n",
    "    \n",
    "    assert cache.get(\"key1\") == \"value1\", \"Basic get/set failed\"\n",
    "    assert cache.get(\"key2\") == \"value2\", \"Basic get/set failed\"\n",
    "    assert cache.size() == 2, f\"Expected size 2, got {cache.size()}\"\n",
    "    \n",
    "    # Test 2: TTL expiration\n",
    "    cache.clear()\n",
    "    cache.set(\"ttl_key\", \"ttl_value\", ttl=1)  # 1 second TTL\n",
    "    assert cache.get(\"ttl_key\") == \"ttl_value\", \"TTL key should be accessible immediately\"\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    assert cache.get(\"ttl_key\") is None, \"TTL key should be expired and return None\"\n",
    "    \n",
    "    # Test 3: Size limits and LRU eviction\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1)\n",
    "    cache.set(\"b\", 2) \n",
    "    cache.set(\"c\", 3)  # Cache is now full (max_size=3)\n",
    "    \n",
    "    # Access 'a' to make it recently used\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4)\n",
    "    \n",
    "    assert cache.get(\"a\") == 1, \"Recently used 'a' should not be evicted\"\n",
    "    assert cache.get(\"b\") is None, \"Least recently used 'b' should be evicted\"\n",
    "    assert cache.get(\"c\") == 3, \"'c' should still be in cache\"\n",
    "    assert cache.get(\"d\") == 4, \"Newly added 'd' should be in cache\"\n",
    "    assert cache.size() == 3, \"Cache size should remain at max_size\"\n",
    "    \n",
    "    # Test 4: Statistics tracking\n",
    "    cache.clear()\n",
    "    cache.set(\"stat_key\", \"stat_value\")\n",
    "    cache.get(\"stat_key\")  # Hit\n",
    "    cache.get(\"nonexistent\")  # Miss\n",
    "    \n",
    "    stats = cache.get_stats()\n",
    "    required_stats = [\"hits\", \"misses\", \"evictions\", \"current_size\"]\n",
    "    for stat in required_stats:\n",
    "        assert stat in stats, f\"Missing statistic: {stat}\"\n",
    "    \n",
    "    assert stats[\"hits\"] > 0, \"Should have recorded hits\"\n",
    "    assert stats[\"misses\"] > 0, \"Should have recorded misses\"\n",
    "    assert stats[\"current_size\"] == 1, \"Should track current size\"\n",
    "    \n",
    "    # Test 5: Manual cleanup\n",
    "    cache.clear()\n",
    "    cache.set(\"expire1\", \"value1\", ttl=1)\n",
    "    cache.set(\"expire2\", \"value2\", ttl=1)\n",
    "    cache.set(\"keep\", \"value3\", ttl=None)  # No expiration\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    \n",
    "    assert removed_count == 2, f\"Should have removed 2 expired items, removed {removed_count}\"\n",
    "    assert cache.get(\"keep\") == \"value3\", \"Non-expiring item should remain\"\n",
    "    assert cache.size() == 1, \"Only one item should remain after cleanup\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    cache.clear()\n",
    "    assert cache.size() == 0, \"Cache should be empty after clear\"\n",
    "    assert cache.get(\"nonexistent\") is None, \"Getting non-existent key should return None\"\n",
    "    assert cache.delete(\"nonexistent\") == False, \"Deleting non-existent key should return False\"\n",
    "    \n",
    "    # Test delete functionality\n",
    "    cache.set(\"delete_me\", \"value\")\n",
    "    assert cache.delete(\"delete_me\") == True, \"Deleting existing key should return True\"\n",
    "    assert cache.get(\"delete_me\") is None, \"Deleted key should not be accessible\"\n",
    "    \n",
    "    print(\"✓ All Question 9 tests passed!\")\n",
    "\n",
    "test_question_9()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391df2c",
   "metadata": {},
   "source": [
    "## Question 10: Integration Challenge (Multiple Components)\n",
    "\n",
    "**Scenario:** You have three separate AI-generated modules that need to work together in a data processing pipeline, but they have interface mismatches and compatibility issues. Your job is to create the integration layer that makes them work together seamlessly.\n",
    "\n",
    "**Requirements:**\n",
    "- Create adapter/wrapper functions to handle data format conversions\n",
    "- Build a unified pipeline that chains all three components\n",
    "- Add comprehensive error handling for the integration\n",
    "- Handle edge cases and invalid data gracefully\n",
    "- Create helper functions for data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a060da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing component integration...\n",
      "\n",
      "=== Testing Individual Components ===\n",
      "DataProcessor output: {'total_items': 1, 'processed_items': [{'id': 'test', 'processed_value': 20, 'original_value': 10, 'status': 'processed'}], 'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}}\n",
      "AnalyticsEngine output: ('Analyzed 1 items (1 successful, 0 failed)', {'avg_value': 20.0, 'max_value': 20, 'min_value': 20, 'total_value': 20, 'success_rate': 1.0})\n",
      "ReportGenerator output:\n",
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 1 items (1 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 20.00\n",
      "    max_value: 20\n",
      "    min_value: 20\n",
      "    total_value: 20\n",
      "    success_rate: 1.00\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Testing Integrated Pipeline ===\n",
      "Processing dataset 1...\n",
      "  Dataset 1 processed successfully\n",
      "Processing dataset 2...\n",
      "  Dataset 2 processed successfully\n",
      "Processing dataset 3...\n",
      "  Dataset 3 processed successfully\n",
      "Processing dataset 4...\n",
      "Processing dataset 5...\n",
      "  Dataset 5 processed successfully\n",
      "Integration successful!\n",
      "==================================================\n",
      "           ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "Section 1: Analyzed 3 items (3 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 40.00\n",
      "    min_value: 20.00\n",
      "    total_value: 90.00\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 2: Analyzed 2 items (2 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 30.00\n",
      "    max_value: 50.00\n",
      "    min_value: 10.00\n",
      "    total_value: 60.00\n",
      "    success_rate: 1.00\n",
      "\n",
      "Section 3: Analyzed 4 items (2 successful, 1 failed)\n",
      "  Metrics:\n",
      "    avg_value: 70.00\n",
      "    max_value: 80.00\n",
      "    min_value: 60.00\n",
      "    total_value: 140.00\n",
      "    success_rate: 0.50\n",
      "\n",
      "Section 4: Analysis failed\n",
      "  Error: No valid data after cleaning\n",
      "\n",
      "Section 5: Analyzed 1 items (1 successful, 0 failed)\n",
      "  Metrics:\n",
      "    avg_value: 200.00\n",
      "    max_value: 200.00\n",
      "    min_value: 200.00\n",
      "    total_value: 200.00\n",
      "    success_rate: 1.00\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "\n",
    "# Component 1: Data Processor (returns dict with specific structure)\n",
    "class DataProcessor:\n",
    "    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n",
    "    \n",
    "    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Process raw data and return structured dict.\"\"\"\n",
    "        if not isinstance(raw_data, list):\n",
    "            raise ValueError(\"Expected list input\")\n",
    "        \n",
    "        result = {\n",
    "            'total_items': len(raw_data),\n",
    "            'processed_items': [],\n",
    "            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n",
    "        }\n",
    "        \n",
    "        for item in raw_data:\n",
    "            if isinstance(item, dict) and 'value' in item:\n",
    "                result['processed_items'].append({\n",
    "                    'id': item.get('id', 'unknown'),\n",
    "                    'processed_value': item['value'] * 2,\n",
    "                    'original_value': item['value'],\n",
    "                    'status': 'processed'\n",
    "                })\n",
    "            else:\n",
    "                result['processed_items'].append({\n",
    "                    'id': 'error',\n",
    "                    'processed_value': 0,\n",
    "                    'original_value': None,\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Component 2: Analytics Engine (expects JSON string, returns tuple)\n",
    "class AnalyticsEngine:\n",
    "    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n",
    "    \n",
    "    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n",
    "        \"\"\"Analyze data from JSON string, return (summary, metrics) tuple.\"\"\"\n",
    "        try:\n",
    "            data = json.loads(json_data_string)\n",
    "        except json.JSONDecodeError:\n",
    "            return None, \"Invalid JSON format\"\n",
    "        \n",
    "        if not isinstance(data, dict) or 'processed_items' not in data:\n",
    "            return None, \"Missing processed_items in data structure\"\n",
    "        \n",
    "        items = data['processed_items']\n",
    "        if not isinstance(items, list):\n",
    "            return None, \"processed_items must be a list\"\n",
    "        \n",
    "        # Extract numeric values for analysis\n",
    "        values = []\n",
    "        failed_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and item.get('status') == 'processed':\n",
    "                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n",
    "                    values.append(item['processed_value'])\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        if not values:\n",
    "            return None, \"No valid numeric data found for analysis\"\n",
    "        \n",
    "        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n",
    "        metrics = {\n",
    "            'avg_value': sum(values) / len(values),\n",
    "            'max_value': max(values),\n",
    "            'min_value': min(values),\n",
    "            'total_value': sum(values),\n",
    "            'success_rate': len(values) / len(items) if items else 0.0\n",
    "        }\n",
    "        \n",
    "        return summary, metrics\n",
    "\n",
    "# Component 3: Report Generator (expects list of tuples, returns formatted string)\n",
    "class ReportGenerator:\n",
    "    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n",
    "    \n",
    "    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n",
    "        \"\"\"Generate report from list of (summary, metrics) tuples.\"\"\"\n",
    "        if not isinstance(analytics_results_list, list):\n",
    "            return \"Error: Expected list input for report generation\"\n",
    "        \n",
    "        if not analytics_results_list:\n",
    "            return \"Error: No data provided for report generation\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"=\" * 50,\n",
    "            \"           ANALYSIS REPORT\",\n",
    "            \"=\" * 50\n",
    "        ]\n",
    "        \n",
    "        for i, result in enumerate(analytics_results_list):\n",
    "            if not isinstance(result, tuple) or len(result) != 2:\n",
    "                report_lines.append(f\"\\nSection {i+1}: Invalid data format - expected (summary, metrics) tuple\")\n",
    "                continue\n",
    "            \n",
    "            summary, metrics = result\n",
    "            \n",
    "            if summary is None:\n",
    "                report_lines.append(f\"\\nSection {i+1}: Analysis failed\")\n",
    "                report_lines.append(f\"  Error: {metrics}\")\n",
    "                continue\n",
    "            \n",
    "            report_lines.append(f\"\\nSection {i+1}: {summary}\")\n",
    "            \n",
    "            if isinstance(metrics, dict):\n",
    "                report_lines.append(\"  Metrics:\")\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        report_lines.append(f\"    {key}: {value:.2f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"    {key}: {value}\")\n",
    "            else:\n",
    "                report_lines.append(f\"  Metrics: {metrics}\")\n",
    "        \n",
    "        report_lines.append(\"\\n\" + \"=\" * 50)\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# Integration functions\n",
    "\n",
    "def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert dictionary to JSON string for AnalyticsEngine.\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Dictionary from DataProcessor\n",
    "        \n",
    "    Returns:\n",
    "        JSON string suitable for AnalyticsEngine\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If data cannot be converted to JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the data is JSON serializable\n",
    "        return json.dumps(data_dict, default=str)  # Use str for non-serializable types\n",
    "    except (TypeError, ValueError) as e:\n",
    "        raise ValueError(f\"Failed to convert data to JSON: {e}\")\n",
    "\n",
    "def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate and clean raw input data.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: Input data of any type\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned list of dictionaries\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If data cannot be converted to the expected format\n",
    "    \"\"\"\n",
    "    # Handle None or empty data\n",
    "    if raw_data is None:\n",
    "        return []\n",
    "    \n",
    "    # Convert single item to list if needed\n",
    "    if not isinstance(raw_data, list):\n",
    "        raw_data = [raw_data]\n",
    "    \n",
    "    cleaned_data = []\n",
    "    \n",
    "    for item in raw_data:\n",
    "        # Skip None items\n",
    "        if item is None:\n",
    "            continue\n",
    "            \n",
    "        # Convert to dictionary if it's not already\n",
    "        if isinstance(item, dict):\n",
    "            cleaned_item = item.copy()  # Create a copy to avoid modifying original\n",
    "        else:\n",
    "            # Try to convert non-dict items\n",
    "            try:\n",
    "                if hasattr(item, '__dict__'):\n",
    "                    cleaned_item = item.__dict__\n",
    "                elif hasattr(item, '_asdict'):  # Handle namedtuples\n",
    "                    cleaned_item = item._asdict()\n",
    "                else:\n",
    "                    cleaned_item = {'value': item, 'id': str(item)}\n",
    "            except (AttributeError, TypeError):\n",
    "                cleaned_item = {'value': 0, 'id': 'conversion_error', 'status': 'converted'}\n",
    "        \n",
    "        # Ensure minimum required structure\n",
    "        if 'id' not in cleaned_item:\n",
    "            cleaned_item['id'] = 'unknown'\n",
    "        \n",
    "        # Validate value field\n",
    "        if 'value' in cleaned_item:\n",
    "            try:\n",
    "                # Convert to number if possible\n",
    "                cleaned_item['value'] = float(cleaned_item['value'])\n",
    "            except (TypeError, ValueError):\n",
    "                # Keep as-is if conversion fails\n",
    "                pass\n",
    "        \n",
    "        cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def integrated_pipeline(raw_data_list: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Integrate all three components to process data end-to-end.\n",
    "    \n",
    "    This function:\n",
    "    1. Validate and clean each raw dataset\n",
    "    2. Process each dataset through DataProcessor\n",
    "    3. Convert results to format expected by AnalyticsEngine\n",
    "    4. Run analytics on each processed dataset\n",
    "    5. Collect all analytics results\n",
    "    6. Generate final report using ReportGenerator\n",
    "    7. Handle all errors gracefully\n",
    "    \n",
    "    Args:\n",
    "        raw_data_list: List of raw data sets to process\n",
    "        \n",
    "    Returns:\n",
    "        str: Final report combining all analyses\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    analytics_results = []\n",
    "    \n",
    "    # Process each dataset\n",
    "    for i, raw_data in enumerate(raw_data_list):\n",
    "        try:\n",
    "            print(f\"Processing dataset {i+1}...\")\n",
    "            \n",
    "            # Step 1: Validate and clean raw data\n",
    "            cleaned_data = validate_and_clean_raw_data(raw_data)\n",
    "            \n",
    "            if not cleaned_data:\n",
    "                analytics_results.append((None, \"No valid data after cleaning\"))\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Process through DataProcessor\n",
    "            processed_data = processor.process_data(cleaned_data)\n",
    "            \n",
    "            # Step 3: Convert to JSON for AnalyticsEngine\n",
    "            json_data = dict_to_json_adapter(processed_data)\n",
    "            \n",
    "            # Step 4: Run analytics\n",
    "            analysis_result = analytics.analyze(json_data)\n",
    "            analytics_results.append(analysis_result)\n",
    "            \n",
    "            print(f\"  Dataset {i+1} processed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any errors gracefully and continue with next dataset\n",
    "            error_msg = f\"Error processing dataset {i+1}: {str(e)}\"\n",
    "            analytics_results.append((None, error_msg))\n",
    "            print(f\"  {error_msg}\")\n",
    "    \n",
    "    # Step 5: Generate final report\n",
    "    return reporter.generate_report(analytics_results)\n",
    "\n",
    "def create_sample_data() -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"Create sample test data for the pipeline.\"\"\"\n",
    "    return [\n",
    "        # Dataset 1: Normal data\n",
    "        [\n",
    "            {'id': 'A1', 'value': 10},\n",
    "            {'id': 'A2', 'value': 20},\n",
    "            {'id': 'A3', 'value': 15}\n",
    "        ],\n",
    "        # Dataset 2: Smaller dataset\n",
    "        [\n",
    "            {'id': 'B1', 'value': 5},\n",
    "            {'id': 'B2', 'value': 25}\n",
    "        ],\n",
    "        # Dataset 3: Mixed data with issues\n",
    "        [\n",
    "            {'id': 'C1', 'value': 30},\n",
    "            {'id': 'C2'},  # Missing value\n",
    "            {'value': 40},  # Missing id\n",
    "            {'id': 'C4', 'value': 'invalid'},  # Invalid value type\n",
    "        ],\n",
    "        # Dataset 4: Edge cases\n",
    "        [],\n",
    "        # Dataset 5: Single item (not in list)\n",
    "        {'id': 'E1', 'value': 100}\n",
    "    ]\n",
    "\n",
    "# Additional helper functions for data transformation\n",
    "\n",
    "def flatten_nested_data(nested_data: List[List[Any]]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Flatten nested data structures into a single list.\n",
    "    \n",
    "    Args:\n",
    "        nested_data: List of lists or mixed data structures\n",
    "        \n",
    "    Returns:\n",
    "        Flattened list of items\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    for item in nested_data:\n",
    "        if isinstance(item, list):\n",
    "            flattened.extend(item)\n",
    "        else:\n",
    "            flattened.append(item)\n",
    "    return flattened\n",
    "\n",
    "def extract_metrics_summary(analytics_results: List[Tuple[Optional[str], Union[Dict, str]]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract and summarize metrics from all analytics results.\n",
    "    \n",
    "    Args:\n",
    "        analytics_results: List of analytics results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with summary statistics\n",
    "    \"\"\"\n",
    "    successful_metrics = []\n",
    "    failed_analyses = 0\n",
    "    \n",
    "    for summary, metrics in analytics_results:\n",
    "        if summary is not None and isinstance(metrics, dict):\n",
    "            successful_metrics.append(metrics)\n",
    "        else:\n",
    "            failed_analyses += 1\n",
    "    \n",
    "    if not successful_metrics:\n",
    "        return {\n",
    "            'total_datasets': len(analytics_results),\n",
    "            'successful_analyses': 0,\n",
    "            'failed_analyses': failed_analyses,\n",
    "            'message': 'No successful analyses to summarize'\n",
    "        }\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_values = []\n",
    "    for metrics in successful_metrics:\n",
    "        if 'total_value' in metrics:\n",
    "            all_values.append(metrics['total_value'])\n",
    "    \n",
    "    return {\n",
    "        'total_datasets': len(analytics_results),\n",
    "        'successful_analyses': len(successful_metrics),\n",
    "        'failed_analyses': failed_analyses,\n",
    "        'overall_avg_value': sum(all_values) / len(all_values) if all_values else 0,\n",
    "        'overall_max_value': max(all_values) if all_values else 0,\n",
    "        'overall_min_value': min(all_values) if all_values else 0,\n",
    "        'success_rate': len(successful_metrics) / len(analytics_results) if analytics_results else 0\n",
    "    }\n",
    "# Test the integration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing component integration...\")\n",
    "    \n",
    "    # Test individual components first\n",
    "    print(\"\\n=== Testing Individual Components ===\")\n",
    "    \n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test', 'value': 10}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    print(f\"DataProcessor output: {processed}\")\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    analysis_result = analytics.analyze(json_data)\n",
    "    print(f\"AnalyticsEngine output: {analysis_result}\")\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([analysis_result])\n",
    "    print(f\"ReportGenerator output:\\n{report}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Integrated Pipeline ===\")\n",
    "    \n",
    "    # Test full pipeline\n",
    "    sample_datasets = create_sample_data()\n",
    "    \n",
    "    try:\n",
    "        final_report = integrated_pipeline(sample_datasets)\n",
    "        print(\"Integration successful!\")\n",
    "        print(final_report)\n",
    "    except Exception as e:\n",
    "        print(f\"Integration failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34c3064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing integrated pipeline...\n",
      "Processing dataset 1...\n",
      "  Dataset 1 processed successfully\n",
      "Processing dataset 2...\n",
      "  Dataset 2 processed successfully\n",
      "Processing dataset 3...\n",
      "Processing dataset 1...\n",
      "  Dataset 1 processed successfully\n",
      "Processing dataset 1...\n",
      "  Dataset 1 processed successfully\n",
      "Processing dataset 2...\n",
      "  Dataset 2 processed successfully\n",
      "Processing dataset 3...\n",
      "  Dataset 3 processed successfully\n",
      "✓ All Question 10 tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cell\n",
    "def test_question_10():\n",
    "    print(\"Testing integrated pipeline...\")\n",
    "    \n",
    "    # Test 1: Individual component functionality\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test1', 'value': 10}, {'id': 'test2', 'value': 20}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    \n",
    "    assert isinstance(processed, dict), \"DataProcessor should return dict\"\n",
    "    assert 'total_items' in processed, \"Missing total_items in processed data\"\n",
    "    assert 'processed_items' in processed, \"Missing processed_items in processed data\"\n",
    "    assert processed['total_items'] == 2, \"Should count items correctly\"\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    summary, metrics = analytics.analyze(json_data)\n",
    "    \n",
    "    assert summary is not None, \"Analytics should return valid summary\"\n",
    "    assert isinstance(metrics, dict), \"Analytics should return metrics dict\"\n",
    "    assert 'avg_value' in metrics, \"Missing avg_value in metrics\"\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([(summary, metrics)])\n",
    "    \n",
    "    assert isinstance(report, str), \"Report should be string\"\n",
    "    assert \"ANALYSIS REPORT\" in report, \"Report should contain header\"\n",
    "    assert \"Section 1\" in report, \"Report should contain section\"\n",
    "    \n",
    "    # Test 2: Data validation and cleaning\n",
    "    cleaned_data = validate_and_clean_raw_data([\n",
    "        {'id': 'valid', 'value': 10},\n",
    "        {'value': 20},  # Missing id\n",
    "        {'id': 'invalid'},  # Missing value\n",
    "        'invalid_format'  # Wrong format\n",
    "    ])\n",
    "    \n",
    "    assert isinstance(cleaned_data, list), \"Should return list\"\n",
    "    # Should handle invalid data gracefully\n",
    "    \n",
    "    # Test 3: Integration adapters\n",
    "    test_dict = {'processed_items': [{'processed_value': 10}]}\n",
    "    json_str = dict_to_json_adapter(test_dict)\n",
    "    \n",
    "    assert isinstance(json_str, str), \"Should return JSON string\"\n",
    "    # Should be valid JSON\n",
    "    parsed = json.loads(json_str)\n",
    "    assert parsed == test_dict, \"Should preserve data structure\"\n",
    "    \n",
    "    # Test 4: Full pipeline integration\n",
    "    sample_datasets = [\n",
    "        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}],\n",
    "        [{'id': 'B1', 'value': 5}],\n",
    "        []  # Empty dataset\n",
    "    ]\n",
    "    \n",
    "    final_report = integrated_pipeline(sample_datasets)\n",
    "    \n",
    "    assert isinstance(final_report, str), \"Pipeline should return string report\"\n",
    "    assert \"ANALYSIS REPORT\" in final_report, \"Should contain report header\"\n",
    "    \n",
    "    # Should handle multiple sections\n",
    "    assert \"Section 1\" in final_report, \"Should have first section\"\n",
    "    assert \"Section 2\" in final_report, \"Should have second section\"\n",
    "    \n",
    "    # Test 5: Error handling\n",
    "    # Test with invalid input\n",
    "    error_report = integrated_pipeline([])\n",
    "    assert isinstance(error_report, str), \"Should handle empty input gracefully\"\n",
    "    \n",
    "    # Test with malformed data\n",
    "    malformed_report = integrated_pipeline([[\"not\", \"a\", \"dict\", \"list\"]])\n",
    "    assert isinstance(malformed_report, str), \"Should handle malformed data\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    edge_cases = [\n",
    "        [{'id': 'only_id'}],  # Missing value\n",
    "        [{'value': 42}],      # Missing id\n",
    "        [{}],                 # Empty dict\n",
    "    ]\n",
    "    \n",
    "    edge_report = integrated_pipeline(edge_cases)\n",
    "    assert isinstance(edge_report, str), \"Should handle edge cases\"\n",
    "    assert \"ANALYSIS REPORT\" in edge_report, \"Should still generate report structure\"\n",
    "    \n",
    "    print(\"✓ All Question 10 tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_question_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4fe67",
   "metadata": {},
   "source": [
    "## Final Submission Instructions\n",
    "\n",
    "### Before You Submit:\n",
    "\n",
    "**Code Quality Checklist:**\n",
    "- All test cells pass without errors\n",
    "- Code follows Python best practices and conventions  \n",
    "- Functions include appropriate documentation\n",
    "- Error handling is implemented where required\n",
    "- Edge cases are handled appropriately\n",
    "- Code is clean, readable, and maintainable\n",
    "\n",
    "**Save Your Work:**\n",
    "- **Save all code outputs** - Run all cells and keep the output visible\n",
    "- Save the notebook file (Ctrl+S / Cmd+S)\n",
    "- Verify all your implementations are in the correct code cells\n",
    "- Double-check that test cells show \"tests passed!\" messages\n",
    "\n",
    "### Submission Format:\n",
    "Submit your completed `firstname_lastname.ipynb` file with **all outputs preserved**. We want to see:\n",
    "- Your code implementations\n",
    "- Test results (passed/failed)\n",
    "- Any debugging output or print statements\n",
    "- Cell execution numbers\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
