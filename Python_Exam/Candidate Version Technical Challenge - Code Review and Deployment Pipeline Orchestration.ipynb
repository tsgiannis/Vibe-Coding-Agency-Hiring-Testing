{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": [
    "## Response Part A: \n",
    "\n",
    "High-level goal. Build an AI-augmented system that automates code review → CI/CD → deployment across environments (dev → staging → prod) while preserving human oversight, traceability, and safe rollback.\n",
    "\n",
    "Phases / Steps\n",
    "\n",
    "Code push / PR creation\n",
    "\n",
    "    Event: PR opened/updated.\n",
    "\n",
    "    Data passed: PR metadata (branch, author, files changed, diff, labels), linked issue IDs, target environment(s), commit hashes.\n",
    "\n",
    "Automated pre-review\n",
    "\n",
    "    Event: PR triggers automated linters, static analysis, security scans.\n",
    "\n",
    "    Data passed: Lint results, unit test results, SCA results, vulnerabilities list with severity, coverage numbers.\n",
    "\n",
    "AI-assisted code review\n",
    "\n",
    "    Event: After pre-review passes, an AI agent performs a code-quality review and produces recommended comments, risk score, and suggested reviewers.\n",
    "\n",
    "    Data passed: PR diff, contextual files, test results, historical commit/author risk signals, previous PRs for same files, suggested fix snippets.\n",
    "\n",
    "Human triage / approval\n",
    "\n",
    "    Event: Human reviewers receive summarized AI findings and either approve, request changes, or escalate.\n",
    "\n",
    "    Data passed: AI summary, detailed AI comments, pre-review artifacts, human review decision + comments.\n",
    "\n",
    "CI build & integration tests\n",
    "\n",
    "    Event: Merge to main branch (or a release branch) triggers build and integration pipelines.\n",
    "\n",
    "    Data passed: Artifact metadata (build id, image tags), environment variables, migration scripts.\n",
    "\n",
    "Canary / staging deployment\n",
    "\n",
    "    Event: Deployment to staging or canary subsets.\n",
    "\n",
    "    Data passed: Deployment spec, traffic split, monitoring baseline thresholds, rollback threshold metadata.\n",
    "\n",
    "Production deployment (progressive)\n",
    "\n",
    "    Event: Canary passes → progressive rollout (e.g., 5% → 25% → 100%) with automated health checks and human opt-in/abort hooks.\n",
    "\n",
    "    Data passed: Observability metrics (latency, errors, custom business metrics), rollback triggers, audit logs.\n",
    "\n",
    "Post-deploy verification & report\n",
    "\n",
    "    Event: After stable production deployment, final verification and full report generation.\n",
    "\n",
    "    Data passed: End-to-end metrics, release notes, vulnerability/quality summary, approvals log.\n",
    "\n",
    "Key cross-phase data / traceability\n",
    "\n",
    "    Unique release identifier (release_id) propagated everywhere.\n",
    "\n",
    "    Audit log capturing who or what (agent id) took each action, timestamps, inputs & outputs.\n",
    "\n",
    "    Artifacts (docker image tag, commit SHA) immutable and recorded.\n",
    "\n",
    "    Correlation IDs to tie monitoring/alerts back to PR/release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": [
    "## Response Part B:\n",
    "Agents, responsibilities & safety/back-doors\n",
    "\n",
    "Agents (roles)\n",
    "\n",
    "    Preflight Agent: Runs linters, unit tests, SAST/SCA, and returns structured findings.\n",
    "\n",
    "    Review Agent (AI): Provides suggested review comments, risk score, complexity estimate, and “confidence” level for each recommendation.\n",
    "\n",
    "    Orchestrator Agent: Coordinates CI/CD, activation of environment deployments, and monitors rollout progress. Enforces policy gates.\n",
    "\n",
    "    Monitoring Agent: Collects metric snapshots before/after rollout and computes change points or anomalies.\n",
    "\n",
    "    Notification/Reporting Agent: Sends human-readable summaries to Slack/Teams/email and generates structured reports for audit storage.\n",
    "\n",
    "    Remediation Agent (optional): Proposes automated quick fixes (e.g., linter fixes, dependency updates) but never auto-commits to protected branches without human approval.\n",
    "\n",
    "    Safety / human overseer \"back-doors\"\n",
    "\n",
    "    Mandatory human approval gates for sensitive releases (e.g., security fixes, database migrations) using branch protection rules and “hold” statuses in the orchestrator.\n",
    "\n",
    "    Kill-switch: global manual kill that halts any ongoing automation and triggers an immediate rollback plan. Exposed via admin UI and a dedicated Slack command (e.g., /deploy abort release_id).\n",
    "\n",
    "    Escalation channels: automated alerts escalate to on-call via PagerDuty/Opsgenie if anomaly thresholds are breached.\n",
    "\n",
    "    Reporting webhooks: every agent emits structured reports to a secure audit log (immutable store), and a human-friendly summary to Slack/Teams. Reports include the agent's confidence and the raw evidence that produced the confidence.\n",
    "\n",
    "    Read-only sandbox: for agents suggesting code edits, require PR creation in a non-protected branch for human review rather than direct commit to main.\n",
    "\n",
    "    Time-limited automation tokens: automation runs with short-lived credentials; sensitive operations require scoped, time-bound approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": [
    "## Response Part C:\n",
    "Orchestration, failure modes & human intervention\n",
    "\n",
    "Orchestration pattern\n",
    "\n",
    "    Use an orchestrator service (stateless) that receives Git webhook events and uses a state machine for each PR/release:\n",
    "\n",
    "    States: pending_preflight → preflight_passed/failed → ai_review → human_approve → build → staging → canary → progressive_prod → done/rollback.\n",
    "\n",
    "    State transitions are logged to audit store; an operator can move state manually when required.\n",
    "\n",
    "Failure modes & mitigation\n",
    "\n",
    "    Unit/integration test failures\n",
    "\n",
    "    Action: Fail pipeline, annotate PR with test logs, suggest possible flakiness detection if intermittent.\n",
    "\n",
    "Security scan high severity\n",
    "\n",
    "    Action: Block merge, notify security team, create ticket automatically with reproduction steps. Offer temporary exception workflow only through a documented approval process (e.g., Jira ticket + release manager signoff).\n",
    "\n",
    "Regression during canary\n",
    "\n",
    "    Action: Orchestrator immediately pauses rollout, triggers automatic rollback to previous artifact, notifies the on-call team, opens incident in incident management tool.\n",
    "\n",
    "Telemetry data missing / monitoring agent down\n",
    "\n",
    "    Action: Stop progressive rollout; in absence of telemetry, require manual human approval to continue.\n",
    "\n",
    "AI agent produces low-confidence suggestions\n",
    "\n",
    "    Action: Mark suggestions as low confidence; require human reviewer and make raw evidence available (diff snippets, tests, historical examples).\n",
    "\n",
    "Human-in-loop mechanisms\n",
    "\n",
    "    Slack / Teams approvals: simple interactive messages with Approve / Request Changes / Escalate buttons that map back to orchestrator API.\n",
    "\n",
    "    Web dashboard: release view with metrics + logs + rollback button.\n",
    "\n",
    "    Email digest/report: daily summary of automated decisions and open PRs requiring attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": [
    "## Response Part D:\n",
    "Implementation strategy & tool selection (concrete stack + examples)\n",
    "\n",
    "Principles\n",
    "\n",
    "    Keep humans in the safety loop for critical actions.\n",
    "\n",
    "    Small, composable microservices/agents communicating over well-defined APIs.\n",
    "\n",
    "    Immutable artifacts and strong audit trails.\n",
    "\n",
    "    Use feature flags for production behavior toggles and progressive rollout.\n",
    "\n",
    "Tooling (practical choices used successfully)\n",
    "\n",
    "    Code hosting & PRs: GitHub (GH Actions + Checks API) or GitLab — both provide rich webhooks and checks API.\n",
    "\n",
    "    CI/CD orchestration: GitHub Actions for per-PR checks; ArgoCD / Flux for GitOps continuous delivery, or Spinnaker if heavy orchestration needed.\n",
    "\n",
    "    Orchestrator / Agent runtime: Kubernetes + lightweight services (Flask/FastAPI or Node) or serverless functions for webhook processors.\n",
    "\n",
    "    Feature flags: LaunchDarkly, Unleash, or Flagsmith.\n",
    "\n",
    "    Security scanning: Snyk for dependency scanning, SonarQube for static analysis; integrate both as preflight gates.\n",
    "\n",
    "    Container registry: ECR / GCR / GitHub Container Registry with signed images.\n",
    "\n",
    "    Monitoring: Prometheus + Grafana for infra metrics; Datadog / New Relic if you prefer SaaS with out-of-the-box anomaly detection.\n",
    "\n",
    "    Alerting/On-call: PagerDuty / Opsgenie integrated with monitoring.\n",
    "\n",
    "    Communication: Slack + GitHub Checks + Jira for issue tracking.\n",
    "\n",
    "    Audit & logs: ElasticSearch or a managed audit log store (e.g., S3 with Glacier lifecycle + immutability policies) for long-term retention.\n",
    "\n",
    "Deployment strategy\n",
    "\n",
    "    Canary + progressive rollout: e.g., Istio/Envoy + Argo Rollouts or Kubernetes Deployment strategies.\n",
    "\n",
    "    Blue/Green for DB schema changes where immediate rollback is needed.\n",
    "\n",
    "    Schema migrations: Use safe migration patterns (expand/contract), always with preflight tests on staging.\n",
    "\n",
    "Example GitHub Actions snippet (high level)\n",
    "\n",
    "name: PR Preflight\n",
    "on: [pull_request]\n",
    "jobs:\n",
    "  preflight:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Run linters\n",
    "        run: make lint\n",
    "      - name: Run unit tests\n",
    "        run: make test\n",
    "      - name: Run SCA\n",
    "        uses: snyk/actions@v2\n",
    "      - name: Post results to orchestrator\n",
    "        run: curl -X POST $ORCHESTRATOR_API/reports -d @report.json\n",
    "\n",
    "\n",
    "Example AI Review Agent prompt (short & pragmatic)\n",
    "\n",
    "  System: You are an expert senior software engineer reviewing a pull request. Provide:\n",
    "\n",
    "    A short-risk summary (Low/Medium/High) with reasons.\n",
    "\n",
    "    Up to 6 actionable comments linked to file paths/line ranges.\n",
    "\n",
    "    Any security or dependency concerns.\n",
    "\n",
    "    A confidence score (0–100) and the top 3 evidence items that justify your findings.\n",
    "\n",
    "User: [PR diff], [unit test results], [recent PRs touching same files], [repo coding standards].\n",
    "\n",
    "    Output in JSON: { \"risk\": \"...\", \"comments\": [...], \"issues\": [...], \"confidence\": 87, \"evidence\": [...] }.\n",
    "\n",
    "  Note: Always persist the AI prompt, inputs and outputs to the audit log so a human can reproduce the agent’s reasoning.\n",
    "\n",
    "Operational observability & reporting (the “back-door” you requested)\n",
    "\n",
    "  Multi-layer reporting\n",
    "\n",
    "  Realtime concise notifications (Slack/Teams)\n",
    "\n",
    "  Every significant event sends a human-friendly message with buttons:\n",
    "\n",
    "Example Slack message on PR AI review:\n",
    "\n",
    ":robot_face: AI Review for PR #432 - \"add payments retry\"\n",
    "Risk: MEDIUM (high complexity in payments module)\n",
    "Suggested reviewers: @alice, @ops-oncall\n",
    "Buttons: [View full report] [Request Human Review] [Ignore]\n",
    "\n",
    "\n",
    "For deployments: progressive status messages with links to live metrics and a single-click rollback button.\n",
    "\n",
    "Detailed structured reports (JSON)\n",
    "\n",
    "Stored in an audit S3 bucket or DB with release_id. Includes all agent outputs, test logs, scanners’ raw output, and the exact prompt used.\n",
    "\n",
    "Daily/weekly summaries\n",
    "\n",
    "  Automated digest of high-risk merges, security findings, failures, and trends (e.g., “Top files causing regressions”).\n",
    "\n",
    "    On-demand deep reports\n",
    "\n",
    "    From the release dashboard, any reviewer can download a PDF/HTML report containing the full chain: PR → Preflight → AI review → Test artifacts → Deploy logs → Post-deploy metrics.\n",
    "\n",
    "Human notification & manual override\n",
    "\n",
    "    Slack interactive messages include Approve / Abort / Escalate. Approve triggers automated state transition; Abort triggers orchestrator rollback.\n",
    "\n",
    "    For critical items (e.g., CVE discovered), require a secondary Jira ticket + engineering manager signoff. The orchestrator will allow bypass only if bypass_ticket_id is present and signed by designated approvers; log bypass in audit store.\n",
    "\n",
    "Audit & compliance\n",
    "\n",
    "    All automated decisions stored immutably with cryptographic signing of artifacts (e.g., signed build artifacts). Provide export for compliance teams.\n",
    "\n",
    "Metrics & success criteria (what we measure)\n",
    "\n",
    "    MTTR (mean time to recovery) — aim to reduce via automated rollback + clear runbooks.\n",
    "\n",
    "  Lead time for changes — measured PR open → merged → prod.\n",
    "\n",
    "    Failed deploys % — target < 1% regressions caught in production after rollout improvements.\n",
    "\n",
    "    False positive rate for AI suggestions — track human overrides to improve models.\n",
    "\n",
    "Security vulnerability delta — time to remediate high severity vulnerabilities.\n",
    "\n",
    "  Human interaction rate — % of automations requiring human approval (tunable by safety policy).\n",
    "\n",
    "Runbook & playbooks (examples)\n",
    "\n",
    "  If canary error rate increases > X% over baseline for 5 minutes\n",
    "\n",
    "Orchestrator: pause rollout, scale replica back to previous image.\n",
    "\n",
    "  Notify on-call with summary and link to logs.\n",
    "\n",
    "  If no response within 2 minutes, trigger automated rollback.\n",
    "\n",
    "  If Snyk finds critical vulnerability in a dependency used in production\n",
    "\n",
    "  Block merges to release branch.\n",
    "\n",
    "  Create high-priority Jira ticket and assign to security owner.\n",
    "\n",
    "  Notify release manager; require signoff for any bypass.\n",
    "\n",
    "  Smaller practical design decisions / recommendations\n",
    "\n",
    "Agent confidence + evidence: never allow AI suggestions to be the only input for dangerous operations (DB migrations, infra changes). Use confidence thresholds to require human review.\n",
    "\n",
    "  Idempotent operations: design deployment steps to be idempotent for safe retries.\n",
    "\n",
    "  Short-lived tokens and least privilege: automation uses tokens with the narrowest permissions necessary.\n",
    "\n",
    "  Chaos-resiliency tests: periodically run game days to test orchestrator responses and human runbooks.\n",
    "\n",
    "  Gradual rollout of automation: start with safe, low-impact tasks (e.g., documentation suggestions), then expand agent authority as trust increases and monitoring shows low false positive rate.\n",
    "\n",
    "Appendix — Sample Slack interactive message (JSON payload concept)\n",
    "{\n",
    "  \"text\": \"AI Review for PR #432 - add payments retry\",\n",
    "  \"attachments\": [\n",
    "    {\n",
    "      \"text\": \"Risk: MEDIUM (complex change in payments module)\\nConfidence: 82\\nSuggested reviewers: @alice, @ops-oncall\",\n",
    "      \"actions\": [\n",
    "        { \"type\": \"button\", \"text\": \"View report\", \"url\": \"https://ci.example.com/reports/432\" },\n",
    "        { \"type\": \"button\", \"text\": \"Request human review\", \"value\": \"req_review_432\" },\n",
    "        { \"type\": \"button\", \"text\": \"Assign to on-call\", \"value\": \"assign_oncall_432\" }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Final notes — tradeoffs & a roadmap\n",
    "\n",
    "Tradeoffs: More automation reduces human load but increases risk if monitoring or audits are weak. My approach biases safety first: automation + human oversight until trust is proven.\n",
    "\n",
    "  Roadmap (3 phases):\n",
    "\n",
    "  Phase 1: Integrate preflight checks, AI suggestions as comments, Slack notifications and audit logging.\n",
    "\n",
    "  Phase 2: Orchestrator coordinates CI/CD with canary rollouts and automated rollbacks, feature flag integration.\n",
    "\n",
    "  Phase 3: Tighten security gates (SCA+SAST), advanced anomaly detection, limited auto-remediation with strict approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
